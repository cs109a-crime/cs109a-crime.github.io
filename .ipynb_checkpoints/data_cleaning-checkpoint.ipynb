{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "Import the data and clean for EDA. Drop columns that don't relate to our analysis, drop rows with unusable data or that are not in our time frame (2015-2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Read in Files__ from csv into pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_2019_full    = pd.read_csv('data/property-assessment-fy2019.csv')\n",
    "property_2018_full    = pd.read_csv('data/property-assessment-fy2018.csv')\n",
    "property_2017_full    = pd.read_csv('data/property-assessment-fy2017.csv')\n",
    "property_2016_full    = pd.read_csv('data/property-assessment-fy2016.csv')\n",
    "property_2015_full    = pd.read_csv('data/property-assessment-fy2015.csv')\n",
    "streetlights_full     = pd.read_csv('data/streetlight_locations.csv')\n",
    "crime_incidents_full  = pd.read_csv('data/crime_incident_reports.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Read in 311__ seperately because it takes longer so you don't have to run if not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "incident_reports_full = pd.read_csv('data/311.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Drop Columns__ after careful inspection of the data contained in each dataset, drop columns that will not help in our modeling. Columns were dropped if they had no effect on the outcome of interest (such as indeces or number of fireplaces in a property) or if the information in them was a duplicate (such as location if we were already given longitude and latitude).\n",
    "\n",
    "1. from `streetlamps` drop everything but `Long` and `Lat`\n",
    "2. from `property_assessment` we only care where the property is and what it's valued at so drop everything that doesn't relate\n",
    "3. from `crime_incidents` drop `Location` and the index, since the location information was duplicationg `Long` and `Lat` and the index was not useful for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop everything but lat and long\n",
    "streetlights = streetlights_full.drop(['the_geom','TYPE','OBJECTID'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of columns to save for properties\n",
    "property_cols = ['ST_NUM','ST_NAME','ST_NAME_SUF','UNIT_NUM','ZIPCODE',\n",
    "                 'AV_LAND','AV_BLDG','AV_TOTAL','GROSS_TAX']\n",
    "\n",
    "# drop all columns not in list (keep _ at end of name to show not fully clean yet)\n",
    "property_2019_ = property_2019_full[property_2019_full.columns[property_2019_full.columns.isin(property_cols)]]\n",
    "property_2018_ = property_2018_full[property_2018_full.columns[property_2018_full.columns.isin(property_cols)]]\n",
    "property_2017_ = property_2017_full[property_2017_full.columns[property_2017_full.columns.isin(property_cols)]]\n",
    "property_2016_ = property_2016_full[property_2016_full.columns[property_2016_full.columns.isin(property_cols)]]\n",
    "property_2015_ = property_2015_full[property_2015_full.columns[property_2015_full.columns.isin(property_cols)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of columns to drop for crime incidents\n",
    "crime_cols_drop = ['INCIDENT_NUMBER','UCR_PART','Location']\n",
    "\n",
    "# drop columns and keep only descriptors of crime, date, and location\n",
    "crime_incidents_ = crime_incidents_full.drop(crime_cols_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of columns to drop for civil incident reports\n",
    "incident_cols_drop = ['case_enquiry_id','closure_reason','case_title','subject','reason',\n",
    "                      'queue', 'department', 'submittedphoto', 'closedphoto', 'neighborhood', \n",
    "                      'neighborhood_services_district', 'ward', 'precinct', 'location_street_name',\n",
    "                      'location_zipcode']\n",
    "\n",
    "# drop redundant and unnecessary columns\n",
    "incident_reports_ = incident_reports_full.drop(incident_cols_drop,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Drop Rows__ that would not be usable in the forseeable future. This includes rows that have no predictor data, or no response variable data, in the form of 'nan' or 'none' or in some cases zeros. Careful inspection of each dataset led us to drop the following:\n",
    "1. the `streetlights` dataset had no rows with immediately visible issues\n",
    "2. from `property_assessment` we dropped all rows that had 0 in all four of the price variables, no issues with location were immediately visible\n",
    "3. from `crime_incidents` we dropped if `Lat` and `Long` did not have usable values because it would be hard to get that information just from the street name and it is vital to our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop row if all price values are 0\n",
    "def property_droprows(df):\n",
    "    df_new = df[(df.AV_LAND != 0)  | (df.AV_BLDG != 0) | (df.AV_TOTAL != 0) | (df.GROSS_TAX != 0)]\n",
    "    return(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop property rows for all years\n",
    "property_2019 = property_droprows(property_2019_)\n",
    "property_2018 = property_droprows(property_2018_)\n",
    "property_2017 = property_droprows(property_2017_)\n",
    "property_2016 = property_droprows(property_2016_)\n",
    "property_2015 = property_droprows(property_2015_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with nan long and lat \n",
    "crime_incidents = crime_incidents_.dropna(subset=['Lat','Long'])\n",
    "\n",
    "# drop rows with zero long and lat\n",
    "crime_incidents = crime_incidents[crime_incidents.Lat != 0]\n",
    "crime_incidents = crime_incidents[crime_incidents.Long != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows that  are outside our timeframe (2015-2019) sequentially\n",
    "drop_years = incident_reports_[~incident_reports_.open_dt.str.contains(\"2011\")]\n",
    "drop_years = drop_years[~drop_years.open_dt.str.contains(\"2012\")]\n",
    "drop_years = drop_years[~drop_years.open_dt.str.contains(\"2013\")]\n",
    "drop_years = drop_years[~drop_years.open_dt.str.contains(\"2014\")]\n",
    "\n",
    "# save remaining incident reports \n",
    "incident_reports = drop_years"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
